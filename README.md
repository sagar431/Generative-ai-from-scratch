# Learning Large Language Models from Scratch

This repository contains my journey of learning Large Language Models (LLMs) from scratch. The project is structured to cover various aspects of LLMs, from fundamentals to advanced implementations.

## Project Structure

```
.
├── 01_fundamentals/          # Basic concepts and foundations
│   ├── tokenization/         # Text tokenization implementations
│   ├── embeddings/          # Word and sentence embeddings
│   └── attention/           # Attention mechanism basics
│
├── 02_architecture/         # Core LLM architectural components
│   ├── transformer/         # Transformer architecture implementation
│   ├── positional_encoding/ # Positional encoding methods
│   └── multi_head_attention/# Multi-head attention implementation
│
├── 03_training/            # Training-related implementations
│   ├── datasets/           # Training data and preprocessing
│   ├── loss_functions/     # Various loss functions
│   └── optimizers/         # Optimization algorithms
│
├── 04_models/              # Different model implementations
│   ├── basic_llm/         # Simple LLM implementation
│   ├── gpt_style/         # GPT-style architecture
│   └── experiments/        # Various experimental models
│
├── utils/                  # Utility functions and helpers
│   ├── data_processing/    # Data processing utilities
│   ├── visualization/      # Visualization tools
│   └── evaluation/        # Model evaluation metrics
│
├── notebooks/             # Jupyter notebooks for experiments
│   ├── tutorials/         # Step-by-step tutorials
│   └── experiments/       # Experimental notebooks
│
└── tests/                # Unit tests and integration tests
    ├── unit/             # Unit tests for components
    └── integration/      # Integration tests
```

## Getting Started

1. Each directory contains specific implementations and examples related to its topic
2. Start with the fundamentals section to build a strong foundation
3. Progress through the sections in order for a structured learning experience
4. Use the notebooks for interactive learning and experimentation

## Prerequisites

- Python 3.8+
- PyTorch
- NumPy
- Transformers library (for comparison and reference)

## Contributing

Feel free to contribute by:
1. Adding new implementations
2. Improving existing code
3. Adding more examples and tutorials
4. Fixing bugs or issues

## License

MIT License 