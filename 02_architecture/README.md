# Architecture

This directory contains implementations of core architectural components of Large Language Models.

## Directory Structure

- `transformer/`: Complete transformer architecture implementation
  - Encoder and Decoder blocks
  - Full transformer model
  - Implementation notes and explanations

- `positional_encoding/`: Different positional encoding methods
  - Sinusoidal encoding
  - Learned positional embeddings
  - Relative positional encoding

- `multi_head_attention/`: Multi-head attention implementations
  - Scaled dot-product attention
  - Multi-head attention mechanism
  - Attention visualization tools

## Implementation Goals

1. Build transformer components from scratch
2. Understand attention mechanisms deeply
3. Implement different positional encoding strategies
4. Create modular and reusable components 